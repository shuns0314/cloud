{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List\n",
    "from glob import glob\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from scipy.ndimage.interpolation import rotate\n",
    "import torchvision.transforms as transforms\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "from data_augmentation import *\n",
    "from archs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"マスクの生成→データセット\"\"\"\n",
    "    def __init__(self,\n",
    "                 img_paths: List[np.array],\n",
    "                 mask_paths: List[np.array],\n",
    "                 train=False):\n",
    "        self.img_paths = img_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.train = train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        img_path = self.img_paths[idx]\n",
    "        mask_path = self.mask_paths[idx]\n",
    "        resize = Resize()\n",
    "\n",
    "        image = np.load(img_path)\n",
    "        image = image.astype('float32')/image.max()\n",
    "        if len(image.shape) == 2:\n",
    "            image = np.reshape(image, [image.shape[0], image.shape[1], 1])\n",
    "        image = image.transpose(2, 0, 1) # (channel, width, height) に変換\n",
    "        image = resize(image)\n",
    "\n",
    "        mask = np.load(mask_path) # (channel, width, height) になってる\n",
    "        mask = mask.astype('uint8')\n",
    "        mask = resize(mask)\n",
    "\n",
    "        # 普通にdatasetのtransformでimageとmaskをランダムでtransformかけようとすると、\n",
    "        # imageとmaskそれぞれにrandomがかかるっぽい。\n",
    "        bounding_only = BoundingOnlyDA(rate=1, classes=4)\n",
    "        image, mask = bounding_only(image, mask)\n",
    "\n",
    "\n",
    "        image = torch.from_numpy(image.copy())\n",
    "        mask = torch.from_numpy(mask.copy())\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_paths = glob('inputs/image_gamma_1.1_0926/*')\n",
    "mask_paths = glob('inputs/mask_gamma_1.1_0926/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Dataset(img_paths, mask_paths, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b4\n"
     ]
    }
   ],
   "source": [
    "model = EfficientNetB4NestedUNet(args=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train_data[0][0].view(1,3,256,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 256, 256])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
